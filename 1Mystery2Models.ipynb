{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers torch scikit-learn datasets matplotlib seaborn wordcloud sentence-transformers\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertModel\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    silhouette_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Define the data path\n",
    "data_path = '/notebooks/1429_1.csv'  # Update this path as necessary\n",
    "\n",
    "# ======================\n",
    "# A) Sentiment Classification\n",
    "# ======================\n",
    "\n",
    "# Load the dataset using pandas\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Function to assign sentiment based on the reviews.rating\n",
    "def assign_sentiment(rating: int) -> str:\n",
    "    if rating >= 4:\n",
    "        return 'positive'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# Apply the function to create a new sentiment column\n",
    "df['sentiment'] = df['reviews.rating'].apply(assign_sentiment)\n",
    "\n",
    "# Display the first few rows to verify the sentiment column\n",
    "print(\"Sentiment Classification - First Five Rows:\")\n",
    "print(df[['reviews.rating', 'sentiment']].head())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['reviews.text'].astype(str).tolist(),\n",
    "    df['sentiment'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Load the tokenizer and model for sentiment classification\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Encode the dataset for training and testing\n",
    "train_encodings = sentiment_tokenizer(\n",
    "    train_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "test_encodings = sentiment_tokenizer(\n",
    "    test_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Convert labels to integers for training\n",
    "label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "train_labels_int = [label_mapping[label] for label in train_labels]\n",
    "test_labels_int = [label_mapping[label] for label in test_labels]\n",
    "\n",
    "# Create a custom dataset class\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings: dict, labels: list):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = ReviewDataset(train_encodings, train_labels_int)\n",
    "test_dataset = ReviewDataset(test_encodings, test_labels_int)\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(eval_pred) -> dict:\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(\n",
    "        labels,\n",
    "        predictions,\n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    recall = recall_score(\n",
    "        labels,\n",
    "        predictions,\n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    f1 = f1_score(\n",
    "        labels,\n",
    "        predictions,\n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Training arguments for sentiment classification\n",
    "sentiment_training_args = TrainingArguments(\n",
    "    output_dir='./sentiment_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir='./sentiment_logs',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for sentiment classification\n",
    "sentiment_trainer = Trainer(\n",
    "    model=sentiment_model,\n",
    "    args=sentiment_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the sentiment classification model\n",
    "print(\"\\nRunning sentiment classification training with learning rate: 5e-5\")\n",
    "sentiment_trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "eval_results_test = sentiment_trainer.evaluate()\n",
    "print(\"\\nSentiment Classification - Test Set Evaluation:\")\n",
    "print(f\"Test Accuracy: {eval_results_test['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {eval_results_test['eval_precision']:.4f}\")\n",
    "print(f\"Test Recall: {eval_results_test['eval_recall']:.4f}\")\n",
    "print(f\"Test F1 Score: {eval_results_test['eval_f1']:.4f}\")\n",
    "print(f\"Test Loss: {eval_results_test['eval_loss']:.4f}\")\n",
    "\n",
    "# Evaluate on the training set\n",
    "eval_results_train = sentiment_trainer.evaluate(train_dataset)\n",
    "print(\"\\nSentiment Classification - Training Set Evaluation:\")\n",
    "print(f\"Train Accuracy: {eval_results_train['eval_accuracy']:.4f}\")\n",
    "print(f\"Train Precision: {eval_results_train['eval_precision']:.4f}\")\n",
    "print(f\"Train Recall: {eval_results_train['eval_recall']:.4f}\")\n",
    "print(f\"Train F1 Score: {eval_results_train['eval_f1']:.4f}\")\n",
    "print(f\"Train Loss: {eval_results_train['eval_loss']:.4f}\")\n",
    "\n",
    "# Function to get samples of reviews classified into positive, neutral, and negative\n",
    "def get_review_samples(\n",
    "    dataframe: pd.DataFrame,\n",
    "    sentiment_column: str = 'sentiment',\n",
    "    text_column: str = 'reviews.text',\n",
    "    samples_per_category: int = 4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Extract and print sample reviews for each sentiment category.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing the reviews.\n",
    "        sentiment_column (str): The column name for sentiment labels.\n",
    "        text_column (str): The column name for review texts.\n",
    "        samples_per_category (int): Number of samples to extract per category.\n",
    "    \"\"\"\n",
    "    categories = ['positive', 'neutral', 'negative']\n",
    "    for category in categories:\n",
    "        samples = dataframe[dataframe[sentiment_column] == category][text_column].sample(\n",
    "            n=samples_per_category,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(f\"\\n{category.capitalize()} Reviews Samples:\")\n",
    "        for i, review in enumerate(samples, 1):\n",
    "            print(f\"{i}. {review}\\n\")\n",
    "\n",
    "# Call the function to get samples from the DataFrame\n",
    "get_review_samples(df)\n",
    "\n",
    "# ======================\n",
    "# B) Product Categorization\n",
    "# ======================\n",
    "\n",
    "# Drop rows where 'name' is empty or NaN\n",
    "df = df.dropna(subset=['name'])\n",
    "\n",
    "# Enhanced Text Preprocessing Function\n",
    "def preprocess_text_enhanced(text: str, lower: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text by optionally lowercasing, removing stop words and punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to preprocess.\n",
    "        lower (bool): Whether to convert text to lowercase.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply enhanced preprocessing to the 'name' column\n",
    "df['clean_name'] = df['name'].apply(lambda x: preprocess_text_enhanced(x, lower=True))\n",
    "\n",
    "# Generating Embeddings with DistilBERT\n",
    "tokenizer_bert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model_bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model_bert.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_bert.to(device)\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a DistilBERT embedding for the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to embed.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer_bert(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embedding.flatten()\n",
    "\n",
    "# Generate embeddings for all product names\n",
    "print(\"\\nGenerating embeddings for product names...\")\n",
    "embeddings = np.array([get_embedding(text) for text in df['clean_name']])\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Apply PCA to reduce the dimensionality of the embeddings before K-Means\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "embeddings_pca = pca.fit_transform(embeddings_scaled)\n",
    "\n",
    "# Clustering with K-Means (K=5)\n",
    "k = 5  # Number of clusters\n",
    "kmeans = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "kmeans.fit(embeddings_pca)\n",
    "sil_score = silhouette_score(embeddings_pca, kmeans.labels_)\n",
    "\n",
    "print(f\"\\nFinal Results for K={k}:\")\n",
    "print(f\"Inertia: {kmeans.inertia_:.4f}, Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "# Assign the cluster labels to the DataFrame\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Cluster Mapping\n",
    "cluster_to_category = {\n",
    "    0: 'Tablets & E-readers',               # Example: Kindle and Fire tablets\n",
    "    1: 'Smart Speakers',                    # Example: Echo devices\n",
    "    2: 'Kids Edition Tablets',              # Example: Kids Edition Tablets\n",
    "    3: 'Entertainment Devices',             # Example: Fire TV\n",
    "    4: 'Bluetooth & Charging Accessories'    # Example: chargers and Bluetooth devices\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "df['assigned_category'] = df['cluster'].map(cluster_to_category)\n",
    "\n",
    "# Reorder columns to place 'assigned_category' right after 'name'\n",
    "columns = list(df.columns)\n",
    "name_idx = columns.index('name')\n",
    "columns.insert(name_idx + 1, columns.pop(columns.index('assigned_category')))\n",
    "\n",
    "# Save the final dataset with reordered columns\n",
    "output_file = '/notebooks/clustered_product_full_v5.csv'  # Update this path as necessary\n",
    "df[columns].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nFinal categorized dataset saved to: {output_file}\")\n",
    "\n",
    "# Model Evaluation\n",
    "print(f\"\\nSilhouette Score: {sil_score:.4f}\")\n",
    "print(f\"Inertia: {kmeans.inertia_:.4f}\")\n",
    "\n",
    "# t-SNE Visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(embeddings_scaled)\n",
    "\n",
    "# Create a DataFrame for the t-SNE results\n",
    "tsne_df = pd.DataFrame(tsne_results, columns=['tsne_x', 'tsne_y'])\n",
    "tsne_df['cluster'] = df['cluster']\n",
    "\n",
    "# Enhanced Plotting with Seaborn\n",
    "plt.figure(figsize=(14, 10))\n",
    "palette = sns.color_palette(\"husl\", k)  # Generate a palette with k distinct colors\n",
    "sns.scatterplot(\n",
    "    x='tsne_x',\n",
    "    y='tsne_y',\n",
    "    hue='cluster',\n",
    "    palette=palette,\n",
    "    data=tsne_df,\n",
    "    alpha=0.9,\n",
    "    s=100,          # Marker size\n",
    "    edgecolor='w'   # Marker edge color\n",
    ")\n",
    "\n",
    "# Optionally, add annotations\n",
    "for i in range(tsne_df.shape[0]):\n",
    "    plt.text(\n",
    "        tsne_df['tsne_x'][i],\n",
    "        tsne_df['tsne_y'][i],\n",
    "        tsne_df['cluster'][i],\n",
    "        fontsize=9,\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title('t-SNE Visualization of K-Means Clusters', fontsize=18)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sample Products from Each Category for Inspection\n",
    "samples_per_category = 5  # Number of samples per category\n",
    "\n",
    "# Create a DataFrame to hold the samples\n",
    "category_samples = pd.DataFrame()\n",
    "\n",
    "# Loop through each category and get samples\n",
    "for category in range(k):\n",
    "    samples = df[df['cluster'] == category].sample(\n",
    "        n=min(samples_per_category, len(df[df['cluster'] == category])),\n",
    "        random_state=42\n",
    "    )\n",
    "    category_samples = pd.concat([category_samples, samples])\n",
    "\n",
    "# Save the samples to a CSV file for inspection\n",
    "sample_output_file = '/notebooks/category_samples.csv'  # Update this path as necessary\n",
    "category_samples[['name', 'assigned_category']].to_csv(sample_output_file, index=False)\n",
    "\n",
    "print(f\"\\nSamples from each category saved to: {sample_output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
